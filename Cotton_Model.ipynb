{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2aea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow \n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98bf235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"C:\\Users\\ASUS\\OneDrive - SRM Institute of Science & Technology\\Pictures\\whatsapp\\Green Bot\\Cotton leaves\\40 Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2063b4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3831 images belonging to 6 classes.\n",
      "Found 957 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,          # Normalize pixel values to be between 0 and 1\n",
    "    shear_range=0.2,         # Shear intensity (for data augmentation)\n",
    "    zoom_range=0.2,          # Zoom range (for data augmentation)\n",
    "    horizontal_flip=True,    # Randomly flip images horizontally (for data augmentation)\n",
    "    validation_split=0.2      # Split the data into training and validation sets\n",
    ")\n",
    "\n",
    "# Create training data generator\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    data_path,\n",
    "    target_size=(128, 128),   # Set your desired image size\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # Assuming you have categorical labels\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Create validation data generator\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    data_path,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd27498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), strides=(2, 2), input_shape=(128, 128, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), strides=(2, 2), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), strides=(2, 2), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Change the number of units in the output layer to 6 (number of classes)\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aab47257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "119/119 [==============================] - 59s 483ms/step - loss: 1.3735 - accuracy: 0.4804 - val_loss: 1.8308 - val_accuracy: 0.2295\n",
      "Epoch 2/10\n",
      "119/119 [==============================] - 52s 436ms/step - loss: 1.0269 - accuracy: 0.6144 - val_loss: 1.8001 - val_accuracy: 0.2920\n",
      "Epoch 3/10\n",
      "119/119 [==============================] - 53s 449ms/step - loss: 0.8959 - accuracy: 0.6723 - val_loss: 1.2055 - val_accuracy: 0.5108\n",
      "Epoch 4/10\n",
      "119/119 [==============================] - 65s 544ms/step - loss: 0.7951 - accuracy: 0.7112 - val_loss: 1.6343 - val_accuracy: 0.4731\n",
      "Epoch 5/10\n",
      "119/119 [==============================] - 72s 603ms/step - loss: 0.7048 - accuracy: 0.7494 - val_loss: 0.5602 - val_accuracy: 0.8168\n",
      "Epoch 6/10\n",
      "119/119 [==============================] - 74s 626ms/step - loss: 0.6460 - accuracy: 0.7657 - val_loss: 0.4196 - val_accuracy: 0.8567\n",
      "Epoch 7/10\n",
      "119/119 [==============================] - 70s 585ms/step - loss: 0.5893 - accuracy: 0.7921 - val_loss: 0.5039 - val_accuracy: 0.8233\n",
      "Epoch 8/10\n",
      "119/119 [==============================] - 63s 533ms/step - loss: 0.5382 - accuracy: 0.8099 - val_loss: 0.4309 - val_accuracy: 0.8621\n",
      "Epoch 9/10\n",
      "119/119 [==============================] - 49s 409ms/step - loss: 0.4951 - accuracy: 0.8239 - val_loss: 0.5804 - val_accuracy: 0.8006\n",
      "Epoch 10/10\n",
      "119/119 [==============================] - 44s 367ms/step - loss: 0.4661 - accuracy: 0.8310 - val_loss: 0.4124 - val_accuracy: 0.8524\n"
     ]
    }
   ],
   "source": [
    "# Set batch size and train the model\n",
    "batch_size = 32\n",
    "\n",
    "# Assuming you have defined your data generators as 'train_generator' and 'validation_generator'\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Optionally, you can save the trained model\n",
    "model.save('plant_disease_detection_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f867e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "The detected disease is: Aphids\n"
     ]
    }
   ],
   "source": [
    "img_path = r\"C:\\Users\\ASUS\\Downloads\\cotton input 2.jpeg\"\n",
    "img = image.load_img(img_path, target_size=(128, 128))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array /= 255.0 \n",
    "\n",
    "prediction = model.predict(img_array)\n",
    "\n",
    "class_indices = train_generator.class_indices\n",
    "predicted_class = list(class_indices.keys())[np.argmax(prediction)]\n",
    "print(f'The detected disease is: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21cb2cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'C:\\Users\\ASUS\\Coding Ninjas\\model\\cotton api.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
